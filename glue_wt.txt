# install awscli on the centos7 vm
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

# configure aws
aws configure

# start docker service
sudo systemctl start docker
# verify docker is started
sudo systemctl status docker
# add current user to docker group to avoid having to run docker as sudo
sudo groupadd docker
sudo usermod -aG docker $USER

# add the aws access keys under .bashrc
export AWS_ACCESS_KEY_ID=<aws access key id>
export AWS_SECRET_ACCESS_KEY=<aws secret key>
export WORKSPACE_LOCATION=/home/samar/workspace
export SCRIPT_FILE_NAME=sample.py

# run a program using spark-submit
docker run -it -v ~/.aws:/home/glue_user/.aws -v $WORKSPACE_LOCATION:/home/glue_user/workspace/ -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY -e AWS_REGION=us-east-1 -e DISABLE_SSL=true --rm -p 4040:4040 -p 18080:18080 --name glue_spark_submit amazon/aws-glue-libs:glue_libs_3.0.0_image_01 spark-submit /home/glue_user/workspace/src/$SCRIPT_FILE_NAME

# run the pyspark repl and a glue job
docker run -it  -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY -e AWS_REGION=us-east-1 -e DISABLE_SSL=true --rm -p 4040:4040 -p 18080:18080 --name glue_pyspark amazon/aws-glue-libs:glue_libs_3.0.0_image_01 pyspark

# create a jupyter notebook for the glue transformation
export JUPYTER_WORKSPACE_LOCATION=/home/samar/jupyterws
mkdir -p /home/samar/jupyterws
docker run -it -v ~/.aws:/home/glue_user/.aws -v $JUPYTER_WORKSPACE_LOCATION:/home/glue_user/workspace/jupyter_workspace/ -e AWS_PROFILE=$PROFILE_NAME -e DISABLE_SSL=true --rm -p 4040:4040 -p 18080:18080 -p 8998:8998 -p 8888:8888 --name glue_jupyter_lab amazon/aws-glue-libs:glue_libs_3.0.0_image_01 /home/glue_user/jupyter/jupyter_start.sh

# create a GlueFullAccessPolicy where 
# Find IAM Service, click Policies and then Create policy
# Select glue service
# Actions - check All Glue actions (glue*)
# Resources - click and choose All resources
# Request conditions - MFA authentitaction requirement can be imposed and source ips can be restricted

# in IAM select roles and click Create role
# Let Trusted endity type be AWS Service and from the drop down box choose glue
# click next and choose the policy created in the earlier step
# give it a role name and click Create Role
# also use AmazonS3FullAccess and GlueConsoleFullAccessPolicies

# run a crawler to load the us legislators data into the legislators database in glue data catalog
# The crawler should create  metadata tables:, persons_json, memberships_json, organizations_json, events_json, areas_json, countries_r_json
# and then run the glue code example for joining and relationalizing data
# Add boilerplate script to the development endpoint notebook
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

glueContext = GlueContext(SparkContext.getOrCreate())

# Examine the schemas from the data in the Data Catalog

persons = glueContext.create_dynamic_frame.from_catalog(
             database="shltr-samar",
             table_name="persons_json")
print("Count: ", persons.count())
persons.printSchema()

# view the schema of the memberships_json table
memberships = glueContext.create_dynamic_frame.from_catalog(
                 database="shltr-samar",
                 table_name="memberships_json")
print("Count: ", memberships.count())
memberships.printSchema()

# The organizations are parties and the two chambers of Congress, the Senate and House of Representatives. To view the schema of the organizations_json table, type the following:
orgs = glueContext.create_dynamic_frame.from_catalog(
           database="shltr-samar",
           table_name="organizations_json")
print("Count: ", orgs.count())
orgs.printSchema()

# Filter the data. Keep only the fields that you want, and rename id to org_id
orgs = orgs.drop_fields(['other_names',
                        'identifiers']).rename_field(
                            'id', 'org_id').rename_field(
                               'name', 'org_name')
orgs.toDF().show()

# Type the following to view the organizations that appear in memberships:
memberships.select_fields(['organization_id']).toDF().distinct().show()

# use AWS Glue to join these relational tables and create one full history table of legislator memberships and their corresponding organizations.
# First, join persons and memberships on id and person_id.
# Next, join the result with orgs on org_id and organization_id.
# Then, drop the redundant fields, person_id and org_id.

You can do all these operations in one (extended) line of code
l_history = Join.apply(orgs,
                       Join.apply(persons, memberships, 'id', 'person_id'),
                       'org_id', 'organization_id').drop_fields(['person_id', 'org_id'])
print("Count: ", l_history.count())

# We have the final table that you can use for analysis. You can write it out in a compact, efficient format for analytics—namely Parquet—that you can run SQL over in AWS Glue, Amazon Athena, or Amazon Redshift Spectrum.

# create the destination bucket where we will write the output
# aws s3api create-bucket --bucket glue-legislator-shltr-output --profile s3user

# The following call writes the table across multiple files to support fast parallel reads when doing analysis later:

glueContext.write_dynamic_frame.from_options(frame = l_history,
          connection_type = "s3",
          connection_options = {"path": "s3://glue-legislator-shltr-output/legislator_history"},
          format = "parquet")

# Transform the data for relational databases
# AWS Glue makes it easy to write the data to relational databases , even with semi-structured data. It offers a transform relationalize, which flattens DynamicFrames no matter how complex the objects in the frame might be.

# Using the l_history DynamicFrame in this example, pass in the name of a root table (hist_root) and a temporary working path to relationalize. This returns a DynamicFrameCollection. You can then list the names of the DynamicFrames in that collection:

dfc = l_history.relationalize("hist_root", "s3://glue-legislator-shltr-output/temp-dir/")
dfc.keys()
# Relationalize broke the history table out into six new tables: a root table that contains a record for each object in the DynamicFrame, and auxiliary tables for the arrays. Array handling in relational databases is often suboptimal, especially as those arrays become large. Separating the arrays into different tables makes the queries go much faster.

# look at the separation by examining contact_details:

l_history.select_fields('contact_details').printSchema()
dfc.select('hist_root_contact_details').toDF().where("id = 10 or id = 75").orderBy(['id','index']).show()

# You are now ready to write your data to a connection by cycling through the DynamicFrames one at a time:

df_connection_options = {
    'dbtable': 'table',
    'database': 'salesdb'
}

# create a bucket which can be used for temp outputs. the user running the container/app should have rights to write to the s3 folder


dfc.select('hist_root').toDF().where(
    "contact_details = 10 or contact_details = 75").select(
       ['id', 'given_name', 'family_name', 'contact_details']).show()

# check the redshift connection by reading a redshift table in the glue catalog
sdf = glueContext.create_dynamic_frame.from_catalog(
    database = "shltr-samar", 
    table_name = "redsalessalesdb_public_category", 
    redshift_tmp_dir = 's3://red-staging-samar', 
    catalog_connection='shl-redshift-connection'
    )
sdf.show()

# check using the jdbc endpoint
my_conn_options = {  
    "url": "jdbc:redshift://my-cluster.cun6wpx0iczb.us-east-1.redshift.amazonaws.com:5439/salesdb",
    "dbtable": "sales",
    "user": "awsuser",
    "password": "Admin98919",
    "redshiftTmpDir": 's3://red-staging-samar',
    "aws_iam_role": "put in the iam role here"
}
df = glueContext.create_dynamic_frame_from_options("redshift", my_conn_options)

# use the glue catalog connection to write the transformed tables to redshift
for df_name in dfc.keys():
  m_df = dfc.select(df_name)
  print("Writing to table: ", df_name)
  df_connection_options['dbtable'] = df_name
  glueContext.write_dynamic_frame.from_jdbc_conf(frame = m_df, catalog_connection = 'shl-redshift-connection', connection_options = df_connection_options, redshift_tmp_dir = 's3://red-staging-samar')










